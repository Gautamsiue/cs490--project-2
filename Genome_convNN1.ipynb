import torchvision
import torchvision.datasets as datasets
import torch
import torchvision.transforms as transforms
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import tqdm

import numpy as np

import mod_seqs, FileUtils, constants
from SeqsData import SeqsData



# check if GPU is available or else default to CPU usage.
device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device


# One hot encoding 

print("Reading in test, train, and validation files")
test = FileUtils.readFile(constants.TEST) # which is 25% of data
train = FileUtils.readFile(constants.TRAIN) # which is 70% of data
validation = FileUtils.readFile(constants.VALID) # which is 5% of data

print("Splitting the test, train and validation data into lists")
test = [line.split() for line in test]
train = [line.split() for line in train]
validation = [line.split() for line in validation]

print(len(test), len(train), len(validation))



print("\nDone")


# create test dataset to be fed into the dataloader
np.transpose(test_one_hot)
print(test_one_hot.shape)
sd_test = SeqsData(test_one_hot)

# create train dataset to be fed into dataloader
np.transpose(train_one_hot)
sd_train = SeqsData(train_one_hot)

# create train dataset to be fed into dataloader
np.transpose(valid_one_hot)
sd_valid = SeqsData(valid_one_hot)

# DataLoader Objects are what feed the network the data by means of tensor stacks defined in the SeqsData class.
test_dataloader = DataLoader(sd_test, batch_size=256, shuffle=True)
train_dataloader = DataLoader(sd_train, batch_size=256, shuffle=True)
valid_dataloader = DataLoader(sd_valid, batch_size=256, shuffle=True)



    


class AlexNet(torch.nn.Module):
    def __init__(self):
        # result size = ((size - kernelSize + 2 * padding) / stride) + 1
        super().__init__()

        self.conv1 = torch.nn.Conv1d(in_channels=4, out_channels=64, kernel_size=11) 

        self.pool2 = torch.nn.MaxPool1d(kernel_size=3, stride=2)

        self.conv3 = torch.nn.Conv1d(in_channels=64, out_channels=192, kernel_size=5, padding=2) 

        self.pool4 = torch.nn.MaxPool1d(kernel_size=3, stride=2)
        self.conv5 = torch.nn.Conv1d(in_channels=192, out_channels=384, kernel_size=3, padding=1)

        self.conv6 = torch.nn.Conv1d(in_channels=384, out_channels=256, kernel_size=3, padding=1)

        self.conv7 = torch.nn.Conv1d(in_channels=256, out_channels=256, kernel_size=3, padding=1) 

        self.pool8 = torch.nn.MaxPool1d(kernel_size=3, stride=2)
        self.pool9 = torch.nn.AdaptiveAvgPool1d(6) 
        self.fc1 = torch.nn.Linear(in_features=(6 * 256), out_features=512)

        self.fc2 = torch.nn.Linear(in_features=512, out_features=1024)


    def forward(self, x):
        x = F.relu(self.conv1(x))

        x = self.pool2(x)

        x = F.relu(self.conv3(x))

        x = self.pool4(x)

        x = F.relu(self.conv5(x))

        x = F.relu(self.conv6(x))

        x = F.relu(self.conv7(x))

        x = self.pool8(x)

        x = self.pool9(x)

        x = torch.flatten(x, 1)

        x = F.dropout(x, 0.5)

        x = F.relu(self.fc1(x))

        x = F.dropout(x, 0.5)

        x = F.relu(self.fc2(x))
        
        
        
        return x
    
class   GNet(nn.Module):
    
    def __init__(self):
        
        super().__init__()

        # Layer1
        self.conv1 = nn.Conv1d(in_channels=4, out_channels=96, kernel_size=11, stride=1, padding=1)
    

        # Layer2:
        self.conv2 = nn.Conv1d(in_channels=96, out_channels=96, kernel_size=7, padding=1, stride=2) 
        self.local_response2=nn.LocalResponseNorm(size=5,alpha=0.0001,beta=0.75,k=2)

        # Layer3:
        self.conv3 = nn.Conv1d(in_channels=96, out_channels=96, kernel_size=3, stride=1, padding=1)
        self.pool_layer3 = nn.MaxPool1d(kernel_size=7, stride=2)
        self.local_response = nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2)

        # Layer4:
        self.conv4 = nn.Conv1d(in_channels=96, out_channels=96, kernel_size=7, stride=4, padding=2)

        # Layer5:
        self.conv5=nn.Conv1d(in_channels=96, out_channels=96, kernel_size=1, stride=1)

        # Layer6:
        self.conv6=nn.Conv1d(in_channels=96, out_channels=96, kernel_size=1, stride=2)
        self.pool_layer6=nn.MaxPool1d(kernel_size=1, stride=1) 
       
        
        # Layer7:
        self.conv7=nn.Conv1d(in_channels=96, out_channels=96, kernel_size=3, stride=1, padding=1) 

        # Layer8:
        self.conv8=nn.Conv1d(in_channels=96, out_channels=96, kernel_size=1, stride=1)

        # Layer9:
        self.conv9 = nn.Conv1d(in_channels=96, out_channels=96, kernel_size=2, stride=2)
        
        
        # Layer10:
        self.fc1 = nn.Linear(in_features=(960), out_features=1024)
        
        # Layer11:
        self.fc2 = nn.Linear(in_features=1024, out_features=1024)
             
    def forward(self, x):
        
        # applying ReLU to layer 1
        x = F.relu(self.conv1(x))

        # applying ReLU to layer 2
        x = F.relu(self.conv2(x))

    
        x = self.pool_layer3(F.relu(self.local_response(self.conv3(x))))
        x = F.dropout(x, .5)

        # applying ReLU to layer 4
        x = F.relu(self.conv4(x))

        # applying ReLU to layer 5
        x = F.relu(self.conv5(x))

        
        # applying ReLU to layer 6
        x = F.relu(self.conv6(x))
        x = F.dropout(x, .5)

        # applying ReLU to layer 7
        x = F.relu(self.conv7(x))

        # applying ReLU to layer 8
        x = F.relu(self.conv8(x))

        # applying ReLU to layer 9
        x = F.relu(self.conv9(x))
        x = F.dropout(x, .5)

        x = torch.flatten(x, 1) # flattening all Dimensions
        
        # applying ReLU to layer 10
        x = F.relu(self.fc1(x))
        x = F.dropout(x, .5)

        # applying ReLU to layer 11
        x = torch.ReLU(self.fc2(x))
        # x = F.dropout(x, .5)

    
          
        return x
        




# Instantiating the model 
model = AlexNet()
model.load_state_dict(torch.load("./alexnet.pth"))
model = model.to(device)
optimizer = optim.Adam(params=model.parameters(),lr=0.0001)
loss_fn = nn.CrossEntropyLoss()


def processTraining(model,device, train_dataloader, optimizer, epochs):
    model.train()
    for batch_ids, (seq, classes) in tqdm(enumerate(train_dataloader)):
        classes=classes.type(torch.LongTensor)
        seq, classes=seq.to(device), classes.to(device)
        torch.autograd.set_detect_anomaly(True)     
        optimizer.zero_grad()
        output=model(seq)
        loss = loss_fn(output,classes)            
        loss.backward()
        optimizer.step()
    


def test(model, device, test_dataloader):
    model.eval()
    test_loss=0
    correct=0
    with torch.no_grad():
        for seq,classes in tqdm(test_dataloader):
            seq,classes=seq.to(device), classes.to(device)
            y_hat=model(seq)
            test_loss+=F.nll_loss(y_hat,classes,reduction='sum').item()
            _,y_pred=torch.max(y_hat,1)
            correct+=(y_pred==classes).sum().item()
    
        test_loss/=len(test_dataloader)
        print("\n Test set: Average loss: {:.0f},Accuracy:{}/{} ({:.0f}%)\n".format(
            test_loss,correct,len(test_dataloader.dataset),100.*correct/len(test_dataloader.dataset)))
        print('*'*50)


def confusion_matrix(preds, classes, tp=0, tn=0, fp=0, fn=0):
    for pred, cl in zip(preds, classes):
        if pred == 1 and cl == 1:
            tp += 1
        if pred == 0 and cl == 1:
            fn += 1
        if pred ==  1 and cl == 0:
            fp += 1
        if pred == 0 and cl == 0:
            tn += 1
    return tp, tn, fp, fn


def validation(model, device, valid_dataloader):
    model.eval()
    test_loss=0
    correct=0
    tp, tn, fp, fn = 0, 0, 0, 0
    
    with torch.no_grad():
        for seq,classes in valid_dataloader:
            seq,classes=seq.to(device), classes.to(device)
            y_hat=model(seq)
            test_loss+=F.nll_loss(y_hat,classes, reduction='sum').item()
            _,y_pred=torch.max(y_hat, 1)
            correct+=(y_pred==classes).sum().item()
            tp, tn, fp, fn = confusion_matrix(y_pred, classes, tp, tn, fp, fn)
        test_loss/=len(valid_dataloader)
        print("\n Valid_set: Average loss: {:.0f},Accuracy:{}/{} ({:.0f}%)\n".format(
                test_loss,correct,len(valid_dataloader.dataset),100.*correct/len(valid_dataloader.dataset)))
        print('='*30)
    precision = tp / (tp + fp)
    recall = tp / (tp + fn)
    f1 = 2 * (precision * recall) / (precision + recall)
    print(f"Actual no total: {tn + fp}, actual yes total: {fn + tp}")
    print(f"n = {tp + tn + fp + fn}")
    print(f"Precision: {precision}, Recall: {recall}, F1 Score: {f1}")
    print(f"True positives: {tp}, True negatives: {tn}, False positives: {fp}, False negatives: {fn}")
    print(f"Predicted no total: {tn + fn}, predicted yes total: {tp + fp}")
    
        
        

if __name__=='__main__':
    seed=42
    EPOCHS=8 
    
    for epoch in range(1,EPOCHS+1):
        train(model,device,train_dataloader,optimizer,epoch)
        test(model,device,test_dataloader)
        validation(model, device, valid_dataloader)
        torch.save(model.state_dict(), constants.ALEXNETSAVE + f"{epoch + 8}" + ".pth") # save model for each epoch





model = torch.load("Gnet.pth")
print(model)



